{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Translator import translate_line\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import rdflib\n",
    "import re\n",
    "from Word2Vec import Word2Vec_Evaluation\n",
    "import sys\n",
    "from scipy.stats import spearmanr\n",
    "import requests\n",
    "import numpy as np\n",
    "import enchant\n",
    "import bs4\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reducao', 'casos', 'virus', 'brasil', 'tratamento', 'doenca', 'numero', 'nesta', 'hospital', 'segundo', 'aids', 'hospitais', 'feira', 'medico', 'terca', 'epidemiologico', 'deteccoes', 'divulgado', 'boletim', 'saude', 'registrou', 'medicos']\n"
     ]
    }
   ],
   "source": [
    "words = ['brasil', 'registrou', 'reducao', 'numero', 'deteccoes', 'aids', 'segundo', 'boletim', 'epidemiologico', 'divulgado', 'nesta', 'terca', 'feira', 'saude', 'medico', 'hospital', 'medicos', 'doenca', 'casos', 'aids', 'virus', 'tratamento', 'hospitais']\n",
    "words = list(set(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank words based on Word2Vec centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranked words:  saude hospitais medicos doenca tratamento epidemiologico registrou aids brasil casos hospital nesta segundo numero reducao divulgado virus feira terca boletim deteccoes\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2Vec_Evaluation(model_path=\"w2vmodels/STM_CBOW_65dim\", stem=True)\n",
    "ranked = w2v.rank_topic(\" \".join(words))\n",
    "words,rank = zip(*ranked)\n",
    "print(\"ranked words: \",\" \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['health', 'hospitals', 'medical', 'treatment', 'epidemiological', 'treatment', 'registered', 'aids', 'cases', 'hospital', 'in', 'this', 'second', 'number', 'reduction', 'publicized', 'virus', 'bulletin', 'detections']\n"
     ]
    }
   ],
   "source": [
    "en_words = translate_line(\" \".join(words), \"pt-br\", \"en_US\")\n",
    "en_words = tokenizer.tokenize(en_words.lower())\n",
    "d = enchant.Dict(\"en_US\")\n",
    "only_english=[]\n",
    "for word in en_words:\n",
    "    if(d.check(word)):\n",
    "        only_english.append(word)\n",
    "en_words = only_english\n",
    "print(en_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['health', 'hospitals', 'treatment', 'treatment', 'aids', 'hospital', 'cases', 'number', 'reduction', 'virus', 'bulletin', 'detections']\n"
     ]
    }
   ],
   "source": [
    "en_nouns=[]\n",
    "for word,pos in nltk.pos_tag(en_words):\n",
    "    if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "        en_nouns.append(word)\n",
    "print(en_nouns) \n",
    "en_words = en_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['health', 'hospitals', 'medical', 'treatment', 'epidemiological', 'treatment', 'registered', 'aids', 'cases', 'hospital', 'second', 'number', 'reduction', 'publicized', 'virus', 'bulletin', 'detections']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "final_words = [w for w in en_words if not w in stop_words]\n",
    "print(final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 5 words: health hospitals medical treatment epidemiological\n"
     ]
    }
   ],
   "source": [
    "print(\"TOP 5 words: \"+\" \".join(final_words[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for: health+hospitals+medical+treatment+epidemiological\n",
      "looking for: health+hospitals+medical+treatment\n",
      "looking for: hospitals+medical+treatment+epidemiological\n",
      "looking for: health+hospitals+medical\n",
      "looking for: hospitals+medical+treatment\n",
      "looking for: medical+treatment+epidemiological\n",
      "looking for: health+hospitals+medical\n",
      "looking for: health+hospitals\n",
      "looking for: hospitals+medical\n",
      "looking for: medical+treatment\n",
      "looking for: treatment+epidemiological\n"
     ]
    }
   ],
   "source": [
    "found_onto = defaultdict(list)\n",
    "word = \"+\".join(final_words[:5])\n",
    "print(\"looking for:\", word)\n",
    "r = requests.get(r\"http://swoogle.umbc.edu/2006/index.php?option=com_frontpage&service=search&queryType=search_swd_ontology&searchString={}&searchStart=1\".format(word))\n",
    "soup = bs4.BeautifulSoup(r.text)\n",
    "links = soup.findAll('a')\n",
    "for link in links:\n",
    "    if(\"class\" in dict(link.attrs).keys() and link.attrs['class'][0]=='external'):\n",
    "        found_onto[word].append(link.attrs['href'])\n",
    "\n",
    "word = \"+\".join(final_words[:4])\n",
    "print(\"looking for:\", word)\n",
    "r = requests.get(r\"http://swoogle.umbc.edu/2006/index.php?option=com_frontpage&service=search&queryType=search_swd_ontology&searchString={}&searchStart=1\".format(word))\n",
    "soup = bs4.BeautifulSoup(r.text)\n",
    "links = soup.findAll('a')\n",
    "for link in links:\n",
    "    if(\"class\" in dict(link.attrs).keys() and link.attrs['class'][0]=='external'):\n",
    "        found_onto[word].append(link.attrs['href'])\n",
    "\n",
    "word = \"+\".join(final_words[1:5])\n",
    "print(\"looking for:\", word)\n",
    "r = requests.get(r\"http://swoogle.umbc.edu/2006/index.php?option=com_frontpage&service=search&queryType=search_swd_ontology&searchString={}&searchStart=1\".format(word))\n",
    "soup = bs4.BeautifulSoup(r.text)\n",
    "links = soup.findAll('a')\n",
    "for link in links:\n",
    "    if(\"class\" in dict(link.attrs).keys() and link.attrs['class'][0]=='external'):\n",
    "        found_onto[word].append(link.attrs['href'])\n",
    "\n",
    "word = \"+\".join(final_words[:3])\n",
    "print(\"looking for:\", word)\n",
    "r = requests.get(r\"http://swoogle.umbc.edu/2006/index.php?option=com_frontpage&service=search&queryType=search_swd_ontology&searchString={}&searchStart=1\".format(word))\n",
    "soup = bs4.BeautifulSoup(r.text)\n",
    "links = soup.findAll('a')\n",
    "for link in links:\n",
    "    if(\"class\" in dict(link.attrs).keys() and link.attrs['class'][0]=='external'):\n",
    "        found_onto[word].append(link.attrs['href'])\n",
    "\n",
    "word = \"+\".join(final_words[1:4])\n",
    "print(\"looking for:\", word)\n",
    "r = requests.get(r\"http://swoogle.umbc.edu/2006/index.php?option=com_frontpage&service=search&queryType=search_swd_ontology&searchString={}&searchStart=1\".format(word))\n",
    "soup = bs4.BeautifulSoup(r.text)\n",
    "links = soup.findAll('a')\n",
    "for link in links:\n",
    "    if(\"class\" in dict(link.attrs).keys() and link.attrs['class'][0]=='external'):\n",
    "        found_onto[word].append(link.attrs['href'])\n",
    "\n",
    "word = \"+\".join(final_words[2:5])\n",
    "print(\"looking for:\", word)\n",
    "r = requests.get(r\"http://swoogle.umbc.edu/2006/index.php?option=com_frontpage&service=search&queryType=search_swd_ontology&searchString={}&searchStart=1\".format(word))\n",
    "soup = bs4.BeautifulSoup(r.text)\n",
    "links = soup.findAll('a')\n",
    "for link in links:\n",
    "    if(\"class\" in dict(link.attrs).keys() and link.attrs['class'][0]=='external'):\n",
    "        found_onto[word].append(link.attrs['href'])\n",
    "        \n",
    "word = \"+\".join(final_words[:3])\n",
    "print(\"looking for:\", word)\n",
    "r = requests.get(r\"http://swoogle.umbc.edu/2006/index.php?option=com_frontpage&service=search&queryType=search_swd_ontology&searchString={}&searchStart=1\".format(word))\n",
    "soup = bs4.BeautifulSoup(r.text)\n",
    "links = soup.findAll('a')\n",
    "for link in links:\n",
    "    if(\"class\" in dict(link.attrs).keys() and link.attrs['class'][0]=='external'):\n",
    "        found_onto[word].append(link.attrs['href'])\n",
    "\n",
    "word = \"+\".join(final_words[:2])\n",
    "print(\"looking for:\", word)\n",
    "r = requests.get(r\"http://swoogle.umbc.edu/2006/index.php?option=com_frontpage&service=search&queryType=search_swd_ontology&searchString={}&searchStart=1\".format(word))\n",
    "soup = bs4.BeautifulSoup(r.text)\n",
    "links = soup.findAll('a')\n",
    "for link in links:\n",
    "    if(\"class\" in dict(link.attrs).keys() and link.attrs['class'][0]=='external'):\n",
    "        found_onto[word].append(link.attrs['href'])\n",
    "\n",
    "word = \"+\".join(final_words[1:3])\n",
    "print(\"looking for:\", word)\n",
    "r = requests.get(r\"http://swoogle.umbc.edu/2006/index.php?option=com_frontpage&service=search&queryType=search_swd_ontology&searchString={}&searchStart=1\".format(word))\n",
    "soup = bs4.BeautifulSoup(r.text)\n",
    "links = soup.findAll('a')\n",
    "for link in links:\n",
    "    if(\"class\" in dict(link.attrs).keys() and link.attrs['class'][0]=='external'):\n",
    "        found_onto[word].append(link.attrs['href'])\n",
    "\n",
    "word = \"+\".join(final_words[2:4])\n",
    "print(\"looking for:\", word)\n",
    "r = requests.get(r\"http://swoogle.umbc.edu/2006/index.php?option=com_frontpage&service=search&queryType=search_swd_ontology&searchString={}&searchStart=1\".format(word))\n",
    "soup = bs4.BeautifulSoup(r.text)\n",
    "links = soup.findAll('a')\n",
    "for link in links:\n",
    "    if(\"class\" in dict(link.attrs).keys() and link.attrs['class'][0]=='external'):\n",
    "        found_onto[word].append(link.attrs['href'])\n",
    "\n",
    "word = \"+\".join(final_words[3:5])\n",
    "print(\"looking for:\", word)\n",
    "r = requests.get(r\"http://swoogle.umbc.edu/2006/index.php?option=com_frontpage&service=search&queryType=search_swd_ontology&searchString={}&searchStart=1\".format(word))\n",
    "soup = bs4.BeautifulSoup(r.text)\n",
    "links = soup.findAll('a')\n",
    "for link in links:\n",
    "    if(\"class\" in dict(link.attrs).keys() and link.attrs['class'][0]=='external'):\n",
    "        found_onto[word].append(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://swpatho.ag-nbi.de/context/naics.owl\n",
      "http://www.mindswap.org/2004/multipleOnt/FactoredOntologies/NCI/Ontology19.owl\n"
     ]
    }
   ],
   "source": [
    "ontos = defaultdict()\n",
    "for on in found_onto.keys():\n",
    "    if(len(found_onto[on])>0):\n",
    "        ontos[found_onto[on][0]]=0\n",
    "    if(len(found_onto[on])>1):\n",
    "        ontos[found_onto[on][0]]=0\n",
    "\n",
    "for on in found_onto.keys():\n",
    "    if(len(found_onto[on])>0):\n",
    "        ontos[found_onto[on][0]]+=1\n",
    "    if(len(found_onto[on])>1):\n",
    "        ontos[found_onto[on][0]]+=1\n",
    "ontos = list(ontos.items())\n",
    "ontos.sort(key = lambda x: x[1], reverse=True)\n",
    "print(ontos[0][0])\n",
    "print(ontos[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_URLs = {}\n",
    "onto_URLs['sport'] = \"https://www.bbc.co.uk/ontologies/sport/3.2.ttl\" \n",
    "onto_URLs['wildlife'] = \"https://www.bbc.co.uk/ontologies/wo/1.1.ttl\"\n",
    "onto_URLs['storyline'] = \"https://www.bbc.co.uk/ontologies/storyline/0.3.ttl\"\n",
    "onto_URLs['provenance'] = \"https://www.bbc.co.uk/ontologies/provenance/1.9.ttl\"\n",
    "onto_URLs['programmes'] = \"https://www.bbc.co.uk/ontologies/po/1.1.ttl\"\n",
    "#onto_URLs['politics'] = \"https://www.bbc.co.uk/ontologies/politics/0.9.ttl\"\n",
    "onto_URLs['journalism'] = \"https://www.bbc.co.uk/ontologies/journalism/0.2.ttl\"\n",
    "onto_URLs['food'] = \"https://www.bbc.co.uk/ontologies/fo/1.1.ttl\"\n",
    "onto_URLs['curriculum'] = \"https://www.bbc.co.uk/ontologies/curriculum/1.3.ttl\"\n",
    "onto_URLs['creativework'] = \"https://www.bbc.co.uk/ontologies/creativework/1.11.ttl\"\n",
    "#onto_URLs['coreconcepts'] = \"https://www.bbc.co.uk/ontologies/coreconcepts/1.13.ttl\"\n",
    "onto_URLs['cms'] = \"https://www.bbc.co.uk/ontologies/cms/3.7.ttl\"\n",
    "onto_URLs['business'] = \"https://www.bbc.co.uk/ontologies/business/0.5.ttl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for onto in onto_URLs.keys():\n",
    "    print(\"For\",onto,\"ontology:\")\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(onto_URLs[onto])\n",
    "    hits = 0\n",
    "    for word in final_words:\n",
    "        for i in g.all_nodes():\n",
    "            if(type(i)==rdflib.term.URIRef):\n",
    "                if(re.search(word, i, re.IGNORECASE)!= None):\n",
    "                    hits+=1\n",
    "                    print(i,\"MATCHED WITH:\",word)\n",
    "            if(type(i)==rdflib.term.Literal):\n",
    "                 if(re.search(word, i, re.IGNORECASE)!= None):\n",
    "                     hits+=1\n",
    "                     print(i,\"MATCHED WITH:\",word)\n",
    "    print(\"{} hits!\".format(hits))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wnl = WordNetLemmatizer()\n",
    "# lemmatized = [wnl.lemmatize(i) for i in en_words]\n",
    "# lemmatized.extend(en_words)\n",
    "# lemmatized = list(set(lemmatized))\n",
    "# print(lemmatized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
